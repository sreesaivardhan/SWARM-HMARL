{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a9772ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment and WorkerNetwork successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import rware\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "# Add the root directory to path to find the 'src' folder\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Import your architecture\n",
    "from src.worker_mlp import WorkerNetwork\n",
    "\n",
    "# Initialize Environment\n",
    "env = gym.make(\"rware:rware-tiny-2ag-v2\")\n",
    "print(\"Environment and WorkerNetwork successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e4d2e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with Safe State Access...\n",
      "Episode 0 | Reward: -14.79 | Pos: (1, 0)\n",
      "Step 2000 | Policy Updated\n",
      "Step 4000 | Policy Updated\n",
      "Step 6000 | Policy Updated\n",
      "Step 8000 | Policy Updated\n",
      "Step 10000 | Policy Updated\n",
      "Step 12000 | Policy Updated\n",
      "Step 14000 | Policy Updated\n",
      "Step 16000 | Policy Updated\n",
      "Step 18000 | Policy Updated\n",
      "Step 20000 | Policy Updated\n",
      "Step 22000 | Policy Updated\n",
      "Step 24000 | Policy Updated\n",
      "Episode 50 | Reward: -15.95 | Pos: (3, 0)\n",
      "Step 26000 | Policy Updated\n",
      "Step 28000 | Policy Updated\n",
      "Step 30000 | Policy Updated\n",
      "Step 32000 | Policy Updated\n",
      "Step 34000 | Policy Updated\n",
      "Step 36000 | Policy Updated\n",
      "Step 38000 | Policy Updated\n",
      "Step 40000 | Policy Updated\n",
      "Step 42000 | Policy Updated\n",
      "Step 44000 | Policy Updated\n",
      "Step 46000 | Policy Updated\n",
      "Step 48000 | Policy Updated\n",
      "Step 50000 | Policy Updated\n",
      "Episode 100 | Reward: -22.53 | Pos: (3, 1)\n",
      "Step 52000 | Policy Updated\n",
      "Step 54000 | Policy Updated\n",
      "Step 56000 | Policy Updated\n",
      "Step 58000 | Policy Updated\n",
      "Step 60000 | Policy Updated\n",
      "Step 62000 | Policy Updated\n",
      "Step 64000 | Policy Updated\n",
      "Step 66000 | Policy Updated\n",
      "Step 68000 | Policy Updated\n",
      "Step 70000 | Policy Updated\n",
      "Step 72000 | Policy Updated\n",
      "Step 74000 | Policy Updated\n",
      "Episode 150 | Reward: -21.45 | Pos: (8, 10)\n",
      "Step 76000 | Policy Updated\n",
      "Step 78000 | Policy Updated\n",
      "Step 80000 | Policy Updated\n",
      "Step 82000 | Policy Updated\n",
      "Step 84000 | Policy Updated\n",
      "Step 86000 | Policy Updated\n",
      "Step 88000 | Policy Updated\n",
      "Step 90000 | Policy Updated\n",
      "Step 92000 | Policy Updated\n",
      "Step 94000 | Policy Updated\n",
      "Step 96000 | Policy Updated\n",
      "Step 98000 | Policy Updated\n",
      "Step 100000 | Policy Updated\n",
      "Episode 200 | Reward: -22.66 | Pos: (7, 7)\n",
      "Step 102000 | Policy Updated\n",
      "Step 104000 | Policy Updated\n",
      "Step 106000 | Policy Updated\n",
      "Step 108000 | Policy Updated\n",
      "Step 110000 | Policy Updated\n",
      "Step 112000 | Policy Updated\n",
      "Step 114000 | Policy Updated\n",
      "Step 116000 | Policy Updated\n",
      "Step 118000 | Policy Updated\n",
      "Step 120000 | Policy Updated\n",
      "Step 122000 | Policy Updated\n",
      "Step 124000 | Policy Updated\n",
      "Episode 250 | Reward: -14.14 | Pos: (5, 2)\n",
      "Step 126000 | Policy Updated\n",
      "Step 128000 | Policy Updated\n",
      "Step 130000 | Policy Updated\n",
      "Step 132000 | Policy Updated\n",
      "Step 134000 | Policy Updated\n",
      "Step 136000 | Policy Updated\n",
      "Step 138000 | Policy Updated\n",
      "Step 140000 | Policy Updated\n",
      "Step 142000 | Policy Updated\n",
      "Step 144000 | Policy Updated\n",
      "Step 146000 | Policy Updated\n",
      "Step 148000 | Policy Updated\n",
      "Step 150000 | Policy Updated\n",
      "Episode 300 | Reward: -18.69 | Pos: (0, 0)\n",
      "Step 152000 | Policy Updated\n",
      "Step 154000 | Policy Updated\n",
      "Step 156000 | Policy Updated\n",
      "Step 158000 | Policy Updated\n",
      "Step 160000 | Policy Updated\n",
      "Step 162000 | Policy Updated\n",
      "Step 164000 | Policy Updated\n",
      "Step 166000 | Policy Updated\n",
      "Step 168000 | Policy Updated\n",
      "Step 170000 | Policy Updated\n",
      "Step 172000 | Policy Updated\n",
      "Step 174000 | Policy Updated\n",
      "Episode 350 | Reward: -18.18 | Pos: (5, 5)\n",
      "Step 176000 | Policy Updated\n",
      "Step 178000 | Policy Updated\n",
      "Step 180000 | Policy Updated\n",
      "Step 182000 | Policy Updated\n",
      "Step 184000 | Policy Updated\n",
      "Step 186000 | Policy Updated\n",
      "Step 188000 | Policy Updated\n",
      "Step 190000 | Policy Updated\n",
      "Step 192000 | Policy Updated\n",
      "Step 194000 | Policy Updated\n",
      "Step 196000 | Policy Updated\n",
      "Step 198000 | Policy Updated\n",
      "Step 200000 | Policy Updated\n",
      "Episode 400 | Reward: -19.64 | Pos: (8, 3)\n",
      "Step 202000 | Policy Updated\n",
      "Step 204000 | Policy Updated\n",
      "Step 206000 | Policy Updated\n",
      "Step 208000 | Policy Updated\n",
      "Step 210000 | Policy Updated\n",
      "Step 212000 | Policy Updated\n",
      "Step 214000 | Policy Updated\n",
      "Step 216000 | Policy Updated\n",
      "Step 218000 | Policy Updated\n",
      "Step 220000 | Policy Updated\n",
      "Step 222000 | Policy Updated\n",
      "Step 224000 | Policy Updated\n",
      "Episode 450 | Reward: -19.00 | Pos: (4, 4)\n",
      "Step 226000 | Policy Updated\n",
      "Step 228000 | Policy Updated\n",
      "Step 230000 | Policy Updated\n",
      "Step 232000 | Policy Updated\n",
      "Step 234000 | Policy Updated\n",
      "Step 236000 | Policy Updated\n",
      "Step 238000 | Policy Updated\n",
      "Step 240000 | Policy Updated\n",
      "Step 242000 | Policy Updated\n",
      "Step 244000 | Policy Updated\n",
      "Step 246000 | Policy Updated\n",
      "Step 248000 | Policy Updated\n",
      "Step 250000 | Policy Updated\n",
      "Episode 500 | Reward: -18.86 | Pos: (1, 10)\n",
      "Step 252000 | Policy Updated\n",
      "Step 254000 | Policy Updated\n",
      "Step 256000 | Policy Updated\n",
      "Step 258000 | Policy Updated\n",
      "Step 260000 | Policy Updated\n",
      "Step 262000 | Policy Updated\n",
      "Step 264000 | Policy Updated\n",
      "Step 266000 | Policy Updated\n",
      "Step 268000 | Policy Updated\n",
      "Step 270000 | Policy Updated\n",
      "Step 272000 | Policy Updated\n",
      "Step 274000 | Policy Updated\n",
      "Episode 550 | Reward: -11.29 | Pos: (4, 8)\n",
      "Step 276000 | Policy Updated\n",
      "Step 278000 | Policy Updated\n",
      "Step 280000 | Policy Updated\n",
      "Step 282000 | Policy Updated\n",
      "Step 284000 | Policy Updated\n",
      "Step 286000 | Policy Updated\n",
      "Step 288000 | Policy Updated\n",
      "Step 290000 | Policy Updated\n",
      "Step 292000 | Policy Updated\n",
      "Step 294000 | Policy Updated\n",
      "Step 296000 | Policy Updated\n",
      "Step 298000 | Policy Updated\n",
      "Step 300000 | Policy Updated\n",
      "Episode 600 | Reward: -14.86 | Pos: (6, 2)\n",
      "Step 302000 | Policy Updated\n",
      "Step 304000 | Policy Updated\n",
      "Step 306000 | Policy Updated\n",
      "Step 308000 | Policy Updated\n",
      "Step 310000 | Policy Updated\n",
      "Step 312000 | Policy Updated\n",
      "Step 314000 | Policy Updated\n",
      "Step 316000 | Policy Updated\n",
      "Step 318000 | Policy Updated\n",
      "Step 320000 | Policy Updated\n",
      "Step 322000 | Policy Updated\n",
      "Step 324000 | Policy Updated\n",
      "Episode 650 | Reward: -17.67 | Pos: (5, 3)\n",
      "Step 326000 | Policy Updated\n",
      "Step 328000 | Policy Updated\n",
      "Step 330000 | Policy Updated\n",
      "Step 332000 | Policy Updated\n",
      "Step 334000 | Policy Updated\n",
      "Step 336000 | Policy Updated\n",
      "Step 338000 | Policy Updated\n",
      "Step 340000 | Policy Updated\n",
      "Step 342000 | Policy Updated\n",
      "Step 344000 | Policy Updated\n",
      "Step 346000 | Policy Updated\n",
      "Step 348000 | Policy Updated\n",
      "Step 350000 | Policy Updated\n",
      "Episode 700 | Reward: -20.36 | Pos: (2, 9)\n",
      "Step 352000 | Policy Updated\n",
      "Step 354000 | Policy Updated\n",
      "Step 356000 | Policy Updated\n",
      "Step 358000 | Policy Updated\n",
      "Step 360000 | Policy Updated\n",
      "Step 362000 | Policy Updated\n",
      "Step 364000 | Policy Updated\n",
      "Step 366000 | Policy Updated\n",
      "Step 368000 | Policy Updated\n",
      "Step 370000 | Policy Updated\n",
      "Step 372000 | Policy Updated\n",
      "Step 374000 | Policy Updated\n",
      "Episode 750 | Reward: -15.44 | Pos: (6, 1)\n",
      "Step 376000 | Policy Updated\n",
      "Step 378000 | Policy Updated\n",
      "Step 380000 | Policy Updated\n",
      "Step 382000 | Policy Updated\n",
      "Step 384000 | Policy Updated\n",
      "Step 386000 | Policy Updated\n",
      "Step 388000 | Policy Updated\n",
      "Step 390000 | Policy Updated\n",
      "Step 392000 | Policy Updated\n",
      "Step 394000 | Policy Updated\n",
      "Step 396000 | Policy Updated\n",
      "Step 398000 | Policy Updated\n",
      "Step 400000 | Policy Updated\n",
      "Episode 800 | Reward: -22.17 | Pos: (6, 10)\n",
      "Step 402000 | Policy Updated\n",
      "Step 404000 | Policy Updated\n",
      "Step 406000 | Policy Updated\n",
      "Step 408000 | Policy Updated\n",
      "Step 410000 | Policy Updated\n",
      "Step 412000 | Policy Updated\n",
      "Step 414000 | Policy Updated\n",
      "Step 416000 | Policy Updated\n",
      "Step 418000 | Policy Updated\n",
      "Step 420000 | Policy Updated\n",
      "Step 422000 | Policy Updated\n",
      "Step 424000 | Policy Updated\n",
      "Episode 850 | Reward: -13.49 | Pos: (0, 4)\n",
      "Step 426000 | Policy Updated\n",
      "Step 428000 | Policy Updated\n",
      "Step 430000 | Policy Updated\n",
      "Step 432000 | Policy Updated\n",
      "Step 434000 | Policy Updated\n",
      "Step 436000 | Policy Updated\n",
      "Step 438000 | Policy Updated\n",
      "Step 440000 | Policy Updated\n",
      "Step 442000 | Policy Updated\n",
      "Step 444000 | Policy Updated\n",
      "Step 446000 | Policy Updated\n",
      "Step 448000 | Policy Updated\n",
      "Step 450000 | Policy Updated\n",
      "Episode 900 | Reward: -17.40 | Pos: (9, 1)\n",
      "Step 452000 | Policy Updated\n",
      "Step 454000 | Policy Updated\n",
      "Step 456000 | Policy Updated\n",
      "Step 458000 | Policy Updated\n",
      "Step 460000 | Policy Updated\n",
      "Step 462000 | Policy Updated\n",
      "Step 464000 | Policy Updated\n",
      "Step 466000 | Policy Updated\n",
      "Step 468000 | Policy Updated\n",
      "Step 470000 | Policy Updated\n",
      "Step 472000 | Policy Updated\n",
      "Step 474000 | Policy Updated\n",
      "Episode 950 | Reward: -15.72 | Pos: (8, 4)\n",
      "Step 476000 | Policy Updated\n",
      "Step 478000 | Policy Updated\n",
      "Step 480000 | Policy Updated\n",
      "Step 482000 | Policy Updated\n",
      "Step 484000 | Policy Updated\n",
      "Step 486000 | Policy Updated\n",
      "Step 488000 | Policy Updated\n",
      "Step 490000 | Policy Updated\n",
      "Step 492000 | Policy Updated\n",
      "Step 494000 | Policy Updated\n",
      "Step 496000 | Policy Updated\n",
      "Step 498000 | Policy Updated\n",
      "Step 500000 | Policy Updated\n",
      "Episode 1000 | Reward: -25.13 | Pos: (8, 1)\n",
      "Step 502000 | Policy Updated\n",
      "Step 504000 | Policy Updated\n",
      "Step 506000 | Policy Updated\n",
      "Step 508000 | Policy Updated\n",
      "Step 510000 | Policy Updated\n",
      "Step 512000 | Policy Updated\n",
      "Step 514000 | Policy Updated\n",
      "Step 516000 | Policy Updated\n",
      "Step 518000 | Policy Updated\n",
      "Step 520000 | Policy Updated\n",
      "Step 522000 | Policy Updated\n",
      "Step 524000 | Policy Updated\n",
      "Episode 1050 | Reward: -12.39 | Pos: (9, 3)\n",
      "Step 526000 | Policy Updated\n",
      "Step 528000 | Policy Updated\n",
      "Step 530000 | Policy Updated\n",
      "Step 532000 | Policy Updated\n",
      "Step 534000 | Policy Updated\n",
      "Step 536000 | Policy Updated\n",
      "Step 538000 | Policy Updated\n",
      "Step 540000 | Policy Updated\n",
      "Step 542000 | Policy Updated\n",
      "Step 544000 | Policy Updated\n",
      "Step 546000 | Policy Updated\n",
      "Step 548000 | Policy Updated\n",
      "Step 550000 | Policy Updated\n",
      "Episode 1100 | Reward: -16.75 | Pos: (3, 1)\n",
      "Step 552000 | Policy Updated\n",
      "Step 554000 | Policy Updated\n",
      "Step 556000 | Policy Updated\n",
      "Step 558000 | Policy Updated\n",
      "Step 560000 | Policy Updated\n",
      "Step 562000 | Policy Updated\n",
      "Step 564000 | Policy Updated\n",
      "Step 566000 | Policy Updated\n",
      "Step 568000 | Policy Updated\n",
      "Step 570000 | Policy Updated\n",
      "Step 572000 | Policy Updated\n",
      "Step 574000 | Policy Updated\n",
      "Episode 1150 | Reward: -27.50 | Pos: (1, 0)\n",
      "Step 576000 | Policy Updated\n",
      "Step 578000 | Policy Updated\n",
      "Step 580000 | Policy Updated\n",
      "Step 582000 | Policy Updated\n",
      "Step 584000 | Policy Updated\n",
      "Step 586000 | Policy Updated\n",
      "Step 588000 | Policy Updated\n",
      "Step 590000 | Policy Updated\n",
      "Step 592000 | Policy Updated\n",
      "Step 594000 | Policy Updated\n",
      "Step 596000 | Policy Updated\n",
      "Step 598000 | Policy Updated\n",
      "Step 600000 | Policy Updated\n",
      "Episode 1200 | Reward: -15.11 | Pos: (6, 10)\n",
      "Step 602000 | Policy Updated\n",
      "Step 604000 | Policy Updated\n",
      "Step 606000 | Policy Updated\n",
      "Step 608000 | Policy Updated\n",
      "Step 610000 | Policy Updated\n",
      "Step 612000 | Policy Updated\n",
      "Step 614000 | Policy Updated\n",
      "Step 616000 | Policy Updated\n",
      "Step 618000 | Policy Updated\n",
      "Step 620000 | Policy Updated\n",
      "Step 622000 | Policy Updated\n",
      "Step 624000 | Policy Updated\n",
      "Episode 1250 | Reward: -15.87 | Pos: (7, 7)\n",
      "Step 626000 | Policy Updated\n",
      "Step 628000 | Policy Updated\n",
      "Step 630000 | Policy Updated\n",
      "Step 632000 | Policy Updated\n",
      "Step 634000 | Policy Updated\n",
      "Step 636000 | Policy Updated\n",
      "Step 638000 | Policy Updated\n",
      "Step 640000 | Policy Updated\n",
      "Step 642000 | Policy Updated\n",
      "Step 644000 | Policy Updated\n",
      "Step 646000 | Policy Updated\n",
      "Step 648000 | Policy Updated\n",
      "Step 650000 | Policy Updated\n",
      "Episode 1300 | Reward: -13.06 | Pos: (3, 10)\n",
      "Step 652000 | Policy Updated\n",
      "Step 654000 | Policy Updated\n",
      "Step 656000 | Policy Updated\n",
      "Step 658000 | Policy Updated\n",
      "Step 660000 | Policy Updated\n",
      "Step 662000 | Policy Updated\n",
      "Step 664000 | Policy Updated\n",
      "Step 666000 | Policy Updated\n",
      "Step 668000 | Policy Updated\n",
      "Step 670000 | Policy Updated\n",
      "Step 672000 | Policy Updated\n",
      "Step 674000 | Policy Updated\n",
      "Episode 1350 | Reward: -22.05 | Pos: (1, 2)\n",
      "Step 676000 | Policy Updated\n",
      "Step 678000 | Policy Updated\n",
      "Step 680000 | Policy Updated\n",
      "Step 682000 | Policy Updated\n",
      "Step 684000 | Policy Updated\n",
      "Step 686000 | Policy Updated\n",
      "Step 688000 | Policy Updated\n",
      "Step 690000 | Policy Updated\n",
      "Step 692000 | Policy Updated\n",
      "Step 694000 | Policy Updated\n",
      "Step 696000 | Policy Updated\n",
      "Step 698000 | Policy Updated\n",
      "Step 700000 | Policy Updated\n",
      "Episode 1400 | Reward: -20.00 | Pos: (3, 9)\n",
      "Step 702000 | Policy Updated\n",
      "Step 704000 | Policy Updated\n",
      "Step 706000 | Policy Updated\n",
      "Step 708000 | Policy Updated\n",
      "Step 710000 | Policy Updated\n",
      "Step 712000 | Policy Updated\n",
      "Step 714000 | Policy Updated\n",
      "Step 716000 | Policy Updated\n",
      "Step 718000 | Policy Updated\n",
      "Step 720000 | Policy Updated\n",
      "Step 722000 | Policy Updated\n",
      "Step 724000 | Policy Updated\n",
      "Episode 1450 | Reward: -20.35 | Pos: (3, 1)\n",
      "Step 726000 | Policy Updated\n",
      "Step 728000 | Policy Updated\n",
      "Step 730000 | Policy Updated\n",
      "Step 732000 | Policy Updated\n",
      "Step 734000 | Policy Updated\n",
      "Step 736000 | Policy Updated\n",
      "Step 738000 | Policy Updated\n",
      "Step 740000 | Policy Updated\n",
      "Step 742000 | Policy Updated\n",
      "Step 744000 | Policy Updated\n",
      "Step 746000 | Policy Updated\n",
      "Step 748000 | Policy Updated\n",
      "Step 750000 | Policy Updated\n",
      "Episode 1500 | Reward: -20.14 | Pos: (9, 2)\n",
      "Step 752000 | Policy Updated\n",
      "Step 754000 | Policy Updated\n",
      "Step 756000 | Policy Updated\n",
      "Step 758000 | Policy Updated\n",
      "Step 760000 | Policy Updated\n",
      "Step 762000 | Policy Updated\n",
      "Step 764000 | Policy Updated\n",
      "Step 766000 | Policy Updated\n",
      "Step 768000 | Policy Updated\n",
      "Step 770000 | Policy Updated\n",
      "Step 772000 | Policy Updated\n",
      "Step 774000 | Policy Updated\n",
      "Episode 1550 | Reward: -20.59 | Pos: (8, 2)\n",
      "Step 776000 | Policy Updated\n",
      "Step 778000 | Policy Updated\n",
      "Step 780000 | Policy Updated\n",
      "Step 782000 | Policy Updated\n",
      "Step 784000 | Policy Updated\n",
      "Step 786000 | Policy Updated\n",
      "Step 788000 | Policy Updated\n",
      "Step 790000 | Policy Updated\n",
      "Step 792000 | Policy Updated\n",
      "Step 794000 | Policy Updated\n",
      "Step 796000 | Policy Updated\n",
      "Step 798000 | Policy Updated\n",
      "Step 800000 | Policy Updated\n",
      "Episode 1600 | Reward: -20.64 | Pos: (2, 1)\n",
      "Step 802000 | Policy Updated\n",
      "Step 804000 | Policy Updated\n",
      "Step 806000 | Policy Updated\n",
      "Step 808000 | Policy Updated\n",
      "Step 810000 | Policy Updated\n",
      "Step 812000 | Policy Updated\n",
      "Step 814000 | Policy Updated\n",
      "Step 816000 | Policy Updated\n",
      "Step 818000 | Policy Updated\n",
      "Step 820000 | Policy Updated\n",
      "Step 822000 | Policy Updated\n",
      "Step 824000 | Policy Updated\n",
      "Episode 1650 | Reward: -20.00 | Pos: (3, 9)\n",
      "Step 826000 | Policy Updated\n",
      "Step 828000 | Policy Updated\n",
      "Step 830000 | Policy Updated\n",
      "Step 832000 | Policy Updated\n",
      "Step 834000 | Policy Updated\n",
      "Step 836000 | Policy Updated\n",
      "Step 838000 | Policy Updated\n",
      "Step 840000 | Policy Updated\n",
      "Step 842000 | Policy Updated\n",
      "Step 844000 | Policy Updated\n",
      "Step 846000 | Policy Updated\n",
      "Step 848000 | Policy Updated\n",
      "Step 850000 | Policy Updated\n",
      "Episode 1700 | Reward: -21.44 | Pos: (1, 2)\n",
      "Step 852000 | Policy Updated\n",
      "Step 854000 | Policy Updated\n",
      "Step 856000 | Policy Updated\n",
      "Step 858000 | Policy Updated\n",
      "Step 860000 | Policy Updated\n",
      "Step 862000 | Policy Updated\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m500\u001b[39m):\n\u001b[32m     64\u001b[39m     timestep += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     action = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# Apply actions\u001b[39;00m\n\u001b[32m     68\u001b[39m     all_actions = \u001b[38;5;28mtuple\u001b[39m([action] + [\u001b[32m0\u001b[39m] * (env.unwrapped.n_agents - \u001b[32m1\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mPPOAgent.select_action\u001b[39m\u001b[34m(self, state, storage)\u001b[39m\n\u001b[32m     21\u001b[39m probs = F.softmax(logits, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     22\u001b[39m dist = Categorical(probs)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m action = \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m storage.states.append(state); storage.actions.append(action)\n\u001b[32m     25\u001b[39m storage.logprobs.append(dist.log_prob(action)); storage.values.append(val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SWARM\\venv\\Lib\\site-packages\\torch\\distributions\\categorical.py:143\u001b[39m, in \u001b[36mCategorical.sample\u001b[39m\u001b[34m(self, sample_shape)\u001b[39m\n\u001b[32m    141\u001b[39m     sample_shape = torch.Size(sample_shape)\n\u001b[32m    142\u001b[39m probs_2d = \u001b[38;5;28mself\u001b[39m.probs.reshape(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m._num_events)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m samples_2d = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.T\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d.reshape(\u001b[38;5;28mself\u001b[39m._extended_shape(sample_shape))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class RolloutStorage:\n",
    "    def __init__(self):\n",
    "        self.states, self.actions, self.logprobs, self.rewards, self.is_terminals, self.values = [], [], [], [], [], []\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.states[:], self.actions[:], self.logprobs[:], self.rewards[:], self.is_terminals[:], self.values[:]\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, obs_shape=71, action_dim=5, lr=3e-4, gamma=0.99, eps_clip=0.2):\n",
    "        self.policy = WorkerNetwork(obs_shape, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.policy_old = WorkerNetwork(obs_shape, action_dim)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.gamma, self.eps_clip = gamma, eps_clip\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state, storage):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, val = self.policy_old(state)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        storage.states.append(state); storage.actions.append(action)\n",
    "        storage.logprobs.append(dist.log_prob(action)); storage.values.append(val)\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, storage):\n",
    "        old_states = torch.cat(storage.states).detach()\n",
    "        old_actions = torch.cat(storage.actions).detach()\n",
    "        old_logprobs = torch.cat(storage.logprobs).detach()\n",
    "        returns, discounted_reward = [], 0\n",
    "        for reward, is_terminal in zip(reversed(storage.rewards), reversed(storage.is_terminals)):\n",
    "            if is_terminal: discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            returns.insert(0, discounted_reward)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        values_cat = torch.cat(storage.values).detach().squeeze()\n",
    "        advantages = returns - values_cat\n",
    "        for _ in range(10):\n",
    "            logits, state_values = self.policy(old_states)\n",
    "            dist = Categorical(F.softmax(logits, dim=-1))\n",
    "            logprobs = dist.log_prob(old_actions)\n",
    "            ratios = torch.exp(logprobs - old_logprobs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.mse_loss(state_values.squeeze(), returns)\n",
    "            self.optimizer.zero_grad(); loss.mean().backward(); self.optimizer.step()\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict()); storage.clear()\n",
    "\n",
    "# --- Training Execution ---\n",
    "agent = PPOAgent()\n",
    "storage = RolloutStorage()\n",
    "max_episodes, update_timestep, timestep = 2000, 2000, 0\n",
    "\n",
    "print(\"Starting Training with Safe State Access...\")\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state, info = env.reset()\n",
    "    current_state = state[0]\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for t in range(500):\n",
    "        timestep += 1\n",
    "        action = agent.select_action(current_state, storage)\n",
    "        \n",
    "        # Apply actions\n",
    "        all_actions = tuple([action] + [0] * (env.unwrapped.n_agents - 1))\n",
    "        next_state, reward, terminated, truncated, info = env.step(all_actions)\n",
    "        \n",
    "        # FIX 1: Access agent positions safely from the internal environment state\n",
    "        # rware stores agents in env.unwrapped.agents\n",
    "        agent_obj = env.unwrapped.agents[0]\n",
    "        agent_x, agent_y = agent_obj.x, agent_obj.y\n",
    "        \n",
    "        # Define a goal (e.g., the first shelf location or center)\n",
    "        goal_x, goal_y = 5, 5 \n",
    "        dist = abs(agent_x - goal_x) + abs(agent_y - goal_y)\n",
    "        \n",
    "        # FIX 2: Convert reward list to a float to stop the UserWarning\n",
    "        base_reward = float(reward[0])\n",
    "        \n",
    "        # Shaped Reward calculation\n",
    "        shaped_reward = base_reward - (0.005 * dist) - 0.01\n",
    "        \n",
    "        storage.rewards.append(shaped_reward)\n",
    "        storage.is_terminals.append(terminated)\n",
    "        current_state = next_state[0]\n",
    "        episode_reward += shaped_reward\n",
    "        \n",
    "        if timestep % update_timestep == 0:\n",
    "            agent.update(storage)\n",
    "            print(f\"Step {timestep} | Policy Updated\")\n",
    "            \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "            \n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode} | Reward: {episode_reward:.2f} | Pos: ({agent_x}, {agent_y})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15f908e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Model saved to: D:\\SWARM\\models\\workers\\worker_v1_850k.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. Define the absolute project root to be safe\n",
    "project_root = r\"D:\\SWARM\"\n",
    "\n",
    "# 2. Define the specific target path for the worker model\n",
    "# Using os.path.join is safer for Windows paths\n",
    "save_dir = os.path.join(project_root, \"models\", \"workers\")\n",
    "model_name = \"worker_v1_850k.pth\"\n",
    "full_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "# 3. Create the directory if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 4. Save the trained weights\n",
    "torch.save(agent.policy.state_dict(), full_path)\n",
    "\n",
    "print(f\"Success! Model saved to: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72187eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
