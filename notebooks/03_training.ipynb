{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e4d2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, obs_shape=71, action_dim=5, lr=3e-4, gamma=0.99, eps_clip=0.2):\n",
    "        self.policy = WorkerNetwork(obs_shape, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.policy_old = WorkerNetwork(obs_shape, action_dim)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state, storage):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, val = self.policy_old(state)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        storage.states.append(state)\n",
    "        storage.actions.append(action)\n",
    "        storage.logprobs.append(dist.log_prob(action))\n",
    "        storage.values.append(val)\n",
    "        \n",
    "        return action.item()\n",
    "\n",
    "    def update(self, storage):\n",
    "        # Convert list to tensors\n",
    "        old_states = torch.cat(storage.states).detach()\n",
    "        old_actions = torch.cat(storage.actions).detach()\n",
    "        old_logprobs = torch.cat(storage.logprobs).detach()\n",
    "        \n",
    "        # Calculate Rewards-to-go (Returns) and Advantages\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(storage.rewards), reversed(storage.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            returns.insert(0, discounted_reward)\n",
    "            \n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        advantages = returns - torch.cat(storage.values).detach().squeeze()\n",
    "\n",
    "        # PPO Update Loop (typically 10-20 epochs)\n",
    "        for _ in range(10):\n",
    "            logits, state_values = self.policy(old_states)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            dist = Categorical(probs)\n",
    "            logprobs = dist.log_prob(old_actions)\n",
    "            \n",
    "            # Policy Ratio\n",
    "            ratios = torch.exp(logprobs - old_logprobs)\n",
    "            \n",
    "            # Clipped Objective\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            \n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.mse_loss(state_values.squeeze(), returns)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        storage.clear()\n",
    "\n",
    "class RolloutStorage:\n",
    "    def __init__(self):\n",
    "        self.states, self.actions, self.logprobs, self.rewards, self.is_terminals, self.values = [], [], [], [], [], []\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.states[:], self.actions[:], self.logprobs[:], self.rewards[:], self.is_terminals[:], self.values[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f908e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
